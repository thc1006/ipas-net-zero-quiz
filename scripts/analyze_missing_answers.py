#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æ 266 é¡Œç„¡ç­”æ¡ˆé¡Œç›®çš„ä¸»é¡Œåˆ†å¸ƒèˆ‡é›£åº¦
å»ºç«‹å°‹æ‰¾ç­”æ¡ˆçš„å„ªå…ˆé †åºç­–ç•¥
"""

import json
import sys
from pathlib import Path
from collections import Counter

# Windows ç·¨ç¢¼ä¿®æ­£
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

def load_json(file_path: Path):
    """è¼‰å…¥ JSON æª”æ¡ˆ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def analyze_missing_questions(q719_data, analysis_report):
    """åˆ†æç„¡ç­”æ¡ˆé¡Œç›®çš„åˆ†å¸ƒ"""

    missing_indices = [item['index'] for item in analysis_report['missing_answers']['unmatched_no_answer']]

    print(f"ğŸ“Š åˆ†æ {len(missing_indices)} é¡Œç„¡ç­”æ¡ˆé¡Œç›®...")

    missing_questions = []

    # å¾ gist_items æ”¶é›†
    for item in q719_data.get('gist_items', []):
        if item.get('index') in missing_indices:
            missing_questions.append({
                'index': item['index'],
                'number': item.get('number'),
                'subject': item.get('exam_subject', 'æœªçŸ¥'),
                'question': item.get('stem', '')[:100],
                'source': 'gist'
            })

    # å¾ our_unique_items æ”¶é›†
    for item in q719_data.get('our_unique_items', []):
        if item.get('index') in missing_indices:
            missing_questions.append({
                'index': item['index'],
                'number': item.get('number'),
                'subject': item.get('exam_subject', 'æœªçŸ¥'),
                'question': item.get('stem', '')[:100],
                'source': 'our_unique'
            })

    # ä¸»é¡Œçµ±è¨ˆ
    subject_counter = Counter(q['subject'] for q in missing_questions)
    source_counter = Counter(q['source'] for q in missing_questions)

    # æŒ‰ä¸»é¡Œåˆ†çµ„
    by_subject = {}
    for q in missing_questions:
        subject = q['subject']
        if subject not in by_subject:
            by_subject[subject] = []
        by_subject[subject].append(q)

    return missing_questions, subject_counter, source_counter, by_subject

def identify_keywords(question_text):
    """è­˜åˆ¥é¡Œç›®é—œéµå­—ï¼ˆç°¡å–®ç‰ˆæœ¬ï¼‰"""
    keywords = []

    # ISO æ¨™æº–
    if 'ISO' in question_text or '14064' in question_text or '14067' in question_text:
        keywords.append('ISOæ¨™æº–')

    # GHG ç›¸é—œ
    if 'GHG' in question_text or 'æº«å®¤æ°£é«”' in question_text or 'æ’æ”¾' in question_text:
        keywords.append('GHGæ’æ”¾')

    # ç›¤æŸ¥ç›¸é—œ
    if 'ç›¤æŸ¥' in question_text or 'æŸ¥è­‰' in question_text or 'é©—è­‰' in question_text:
        keywords.append('ç›¤æŸ¥é©—è­‰')

    # ç¢³è¶³è·¡
    if 'ç¢³è¶³è·¡' in question_text or 'CFP' in question_text or 'PCF' in question_text:
        keywords.append('ç¢³è¶³è·¡')

    # åœ‹éš›å”è­°
    if 'å·´é»å”å®š' in question_text or 'COP' in question_text or 'UNFCCC' in question_text:
        keywords.append('åœ‹éš›å”è­°')

    # Scope
    if 'Scope' in question_text or 'ç¯„ç–‡' in question_text:
        keywords.append('æ’æ”¾ç¯„ç–‡')

    # æ·¨é›¶
    if 'æ·¨é›¶' in question_text or 'Net Zero' in question_text or 'ç¢³ä¸­å’Œ' in question_text:
        keywords.append('æ·¨é›¶ç›®æ¨™')

    return keywords if keywords else ['å…¶ä»–']

def prioritize_questions(missing_questions):
    """å»ºç«‹å°‹æ‰¾ç­”æ¡ˆçš„å„ªå…ˆé †åº"""

    prioritized = []

    for q in missing_questions:
        keywords = identify_keywords(q['question'])

        # è¨ˆç®—å„ªå…ˆç´šåˆ†æ•¸
        priority_score = 0

        # ISO æ¨™æº–é¡Œå„ªå…ˆï¼ˆåŸºç¤çŸ¥è­˜ï¼‰
        if 'ISOæ¨™æº–' in keywords:
            priority_score += 10

        # æ’æ”¾ç¯„ç–‡é¡Œå„ªå…ˆï¼ˆæ ¸å¿ƒæ¦‚å¿µï¼‰
        if 'æ’æ”¾ç¯„ç–‡' in keywords:
            priority_score += 8

        # GHG æ’æ”¾é¡Œ
        if 'GHGæ’æ”¾' in keywords:
            priority_score += 7

        # ç›¤æŸ¥é©—è­‰é¡Œ
        if 'ç›¤æŸ¥é©—è­‰' in keywords:
            priority_score += 6

        # ç¢³è¶³è·¡é¡Œ
        if 'ç¢³è¶³è·¡' in keywords:
            priority_score += 5

        # åœ‹éš›å”è­°é¡Œ
        if 'åœ‹éš›å”è­°' in keywords:
            priority_score += 4

        # æ·¨é›¶ç›®æ¨™é¡Œ
        if 'æ·¨é›¶ç›®æ¨™' in keywords:
            priority_score += 3

        prioritized.append({
            **q,
            'keywords': keywords,
            'priority_score': priority_score
        })

    # æŒ‰å„ªå…ˆç´šæ’åº
    prioritized.sort(key=lambda x: x['priority_score'], reverse=True)

    return prioritized

def main():
    base_dir = Path(__file__).parent.parent

    print("="*60)
    print("ğŸ” åˆ†æ 266 é¡Œç„¡ç­”æ¡ˆé¡Œç›®")
    print("="*60)

    # è¼‰å…¥è³‡æ–™
    print("\nğŸ“¥ è¼‰å…¥è³‡æ–™...")
    q719_data = load_json(base_dir / 'quiz-app' / 'src' / 'data' / 'integrated_dataset.json')
    analysis_report = load_json(base_dir / 'DATASET_ANALYSIS_REPORT.json')

    # åˆ†æç„¡ç­”æ¡ˆé¡Œç›®
    missing_questions, subject_counter, source_counter, by_subject = analyze_missing_questions(
        q719_data, analysis_report
    )

    # å»ºç«‹å„ªå…ˆé †åº
    print("\nğŸ¯ å»ºç«‹ç­”æ¡ˆå°‹æ‰¾å„ªå…ˆé †åº...")
    prioritized = prioritize_questions(missing_questions)

    # ç”Ÿæˆå ±å‘Š
    report = {
        'summary': {
            'total_missing': len(missing_questions),
            'by_subject': dict(subject_counter),
            'by_source': dict(source_counter)
        },
        'by_subject': {
            subject: [{'index': q['index'], 'question': q['question']}
                     for q in questions]
            for subject, questions in by_subject.items()
        },
        'prioritized': prioritized,  # æ‰€æœ‰é¡Œç›®ï¼Œå·²æŒ‰å„ªå…ˆç´šæ’åº
        'keyword_distribution': {}
    }

    # é—œéµå­—çµ±è¨ˆ
    all_keywords = []
    for q in prioritized:
        all_keywords.extend(q['keywords'])
    keyword_counter = Counter(all_keywords)
    report['keyword_distribution'] = dict(keyword_counter)

    # å„²å­˜å ±å‘Š
    output_file = base_dir / 'MISSING_ANSWERS_ANALYSIS.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    # è¼¸å‡ºçµ±è¨ˆ
    print("\n" + "="*60)
    print("ğŸ“Š ç„¡ç­”æ¡ˆé¡Œç›®çµ±è¨ˆ")
    print("="*60)
    print(f"\nç¸½é¡Œæ•¸: {len(missing_questions)}")

    print("\nğŸ“š æŒ‰è€ƒç§‘åˆ†å¸ƒ:")
    for subject, count in subject_counter.most_common():
        print(f"  {subject}: {count} é¡Œ")

    print("\nğŸ“¦ æŒ‰ä¾†æºåˆ†å¸ƒ:")
    for source, count in source_counter.most_common():
        source_name = 'Gist' if source == 'gist' else 'ç¨ç‰¹é¡Œç›®'
        print(f"  {source_name}: {count} é¡Œ")

    print("\nğŸ·ï¸  é—œéµå­—åˆ†å¸ƒ:")
    for keyword, count in keyword_counter.most_common(10):
        print(f"  {keyword}: {count} é¡Œ")

    print(f"\nğŸ“ å ±å‘Šå·²å„²å­˜: {output_file}")
    print("="*60)

    # å„ªå…ˆè™•ç†å»ºè­°
    print("\nğŸ¯ å„ªå…ˆè™•ç†å»ºè­° (å‰ 10 é¡Œ):")
    print("-"*60)
    for i, q in enumerate(prioritized[:10], 1):
        print(f"\n{i}. [å„ªå…ˆç´š: {q['priority_score']}] {q['subject']}")
        print(f"   å•é¡Œ: {q['question']}")
        print(f"   é—œéµå­—: {', '.join(q['keywords'])}")

if __name__ == '__main__':
    main()
