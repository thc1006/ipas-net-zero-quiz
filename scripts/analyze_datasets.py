#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æ 719 é¡Œèˆ‡ 310 é¡Œçš„å°æ‡‰é—œä¿‚
æ‰¾å‡ºé‡ç–Šã€ç¨ç‰¹é¡Œç›®ï¼Œä»¥åŠç¼ºå¤±ç­”æ¡ˆçš„é¡Œç›®
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Set

# Windows ç·¨ç¢¼ä¿®æ­£
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

def load_json(file_path: Path) -> any:
    """è¼‰å…¥ JSON æª”æ¡ˆ"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def normalize_question(text: str) -> str:
    """æ­£è¦åŒ–é¡Œç›®æ–‡å­—ï¼ˆç§»é™¤ç©ºç™½ã€æ¨™é»å·®ç•°ï¼‰"""
    return ''.join(text.split()).lower()

def extract_310_questions(data: List[Dict]) -> Dict[str, Dict]:
    """å¾ 310 é¡Œè³‡æ–™é›†æå–é¡Œç›®ï¼ˆä½¿ç”¨ ID ä½œç‚º keyï¼‰"""
    questions = {}
    for item in data:
        questions[item['id']] = {
            'id': item['id'],
            'subject': item['subject'],
            'question': item['question'],
            'answer': item['answer'],
            'normalized': normalize_question(item['question'])
        }
    return questions

def extract_719_questions(data: Dict) -> Dict[int, Dict]:
    """å¾ 719 é¡Œè³‡æ–™é›†æå–é¡Œç›®ï¼ˆä½¿ç”¨ index ä½œç‚º keyï¼‰"""
    questions = {}

    # Gist items
    for item in data.get('gist_items', []):
        idx = item.get('index', item.get('number', len(questions) + 1))
        questions[idx] = {
            'index': idx,
            'number': item.get('number', idx),
            'subject': item.get('exam_subject', item.get('original_section', 'æœªçŸ¥')),
            'question': item.get('stem', ''),
            'answer': item.get('answer'),
            'source': 'gist',
            'normalized': normalize_question(item.get('stem', ''))
        }

    # Our unique items
    for item in data.get('our_unique_items', []):
        idx = item.get('index', item.get('number', len(questions) + 1))
        questions[idx] = {
            'index': idx,
            'number': item.get('number', idx),
            'subject': item.get('exam_subject', item.get('original_section', 'æœªçŸ¥')),
            'question': item.get('stem', ''),
            'answer': item.get('answer'),
            'source': 'our_unique',
            'normalized': normalize_question(item.get('stem', ''))
        }

    return questions

def find_matches(q310: Dict[str, Dict], q719: Dict[int, Dict]) -> Dict:
    """æ‰¾å‡º 310 é¡Œèˆ‡ 719 é¡Œçš„å°æ‡‰é—œä¿‚"""

    matches = {
        'exact_id_match': [],  # ID å®Œå…¨åŒ¹é…ï¼ˆå¦‚æœ 719 æœ‰ ID æ¬„ä½ï¼‰
        'text_match': [],      # é¡Œç›®æ–‡å­—åŒ¹é…
        'no_match_310': [],    # 310 é¡Œä¸­ç„¡æ³•åœ¨ 719 æ‰¾åˆ°çš„
        'no_match_719': []     # 719 é¡Œä¸­ç„¡æ³•åœ¨ 310 æ‰¾åˆ°çš„
    }

    # å»ºç«‹ 719 é¡Œçš„æ­£è¦åŒ–æ–‡å­—ç´¢å¼•
    q719_normalized = {v['normalized']: k for k, v in q719.items()}

    # æª¢æŸ¥ 310 é¡Œåœ¨ 719 ä¸­çš„åŒ¹é…
    matched_310 = set()
    matched_719 = set()

    for q310_id, q310_item in q310.items():
        normalized = q310_item['normalized']

        if normalized in q719_normalized:
            q719_index = q719_normalized[normalized]
            matches['text_match'].append({
                'q310_id': q310_id,
                'q719_index': q719_index,
                'q310_answer': q310_item['answer'],
                'q719_answer': q719[q719_index]['answer'],
                'answer_status': 'match' if q310_item['answer'] == q719[q719_index]['answer']
                                else 'conflict' if q719[q719_index]['answer']
                                else 'need_update'
            })
            matched_310.add(q310_id)
            matched_719.add(q719_index)
        else:
            matches['no_match_310'].append({
                'id': q310_id,
                'question': q310_item['question'][:100]
            })

    # æ‰¾å‡º 719 ä¸­æœªåŒ¹é…çš„é¡Œç›®
    for q719_index, q719_item in q719.items():
        if q719_index not in matched_719:
            matches['no_match_719'].append({
                'index': q719_index,
                'subject': q719_item['subject'],
                'question': q719_item['question'][:100],
                'has_answer': q719_item['answer'] is not None,
                'source': q719_item['source']
            })

    return matches

def analyze_missing_answers(matches: Dict, q719: Dict[int, Dict]) -> Dict:
    """åˆ†æç¼ºå¤±ç­”æ¡ˆçš„é¡Œç›®"""

    missing_answers = {
        'matched_need_update': [],  # 719 ä¸­æœ‰å°æ‡‰ä½†ç¼ºç­”æ¡ˆçš„
        'unmatched_no_answer': [],  # 719 ä¸­ç„¡å°æ‡‰ä¸”ç„¡ç­”æ¡ˆçš„
        'unmatched_has_answer': []  # 719 ä¸­ç„¡å°æ‡‰ä½†æœ‰ç­”æ¡ˆçš„
    }

    # å·²åŒ¹é…ä½†éœ€è¦æ›´æ–°ç­”æ¡ˆçš„
    for match in matches['text_match']:
        if match['answer_status'] == 'need_update':
            q719_index = match['q719_index']
            missing_answers['matched_need_update'].append({
                'q719_index': q719_index,
                'q310_id': match['q310_id'],
                'question': q719[q719_index]['question'][:100],
                'q310_answer': match['q310_answer']
            })

    # æœªåŒ¹é…çš„é¡Œç›®
    for item in matches['no_match_719']:
        if not item['has_answer']:
            missing_answers['unmatched_no_answer'].append(item)
        else:
            missing_answers['unmatched_has_answer'].append(item)

    return missing_answers

def main():
    base_dir = Path(__file__).parent.parent

    # è¼‰å…¥è³‡æ–™
    print("ğŸ“¥ è¼‰å…¥è³‡æ–™é›†...")
    q310_data = load_json(base_dir / 'quiz-app' / 'src' / 'data' / 'questions.json')
    q719_data = load_json(base_dir / 'quiz-app' / 'src' / 'data' / 'integrated_dataset.json')

    # æå–é¡Œç›®
    print("ğŸ” æå–é¡Œç›®...")
    q310 = extract_310_questions(q310_data)
    q719 = extract_719_questions(q719_data)

    print(f"âœ… 310 é¡Œè³‡æ–™: {len(q310)} é¡Œ")
    print(f"âœ… 719 é¡Œè³‡æ–™: {len(q719)} é¡Œ")

    # æ‰¾å‡ºå°æ‡‰é—œä¿‚
    print("\nğŸ”— åˆ†æå°æ‡‰é—œä¿‚...")
    matches = find_matches(q310, q719)

    # åˆ†æç¼ºå¤±ç­”æ¡ˆ
    print("ğŸ“Š åˆ†æç¼ºå¤±ç­”æ¡ˆ...")
    missing = analyze_missing_answers(matches, q719)

    # ç”Ÿæˆå ±å‘Š
    report = {
        'summary': {
            'q310_total': len(q310),
            'q719_total': len(q719),
            'text_matched': len(matches['text_match']),
            'no_match_310': len(matches['no_match_310']),
            'no_match_719': len(matches['no_match_719']),
            'matched_need_update': len(missing['matched_need_update']),
            'unmatched_no_answer': len(missing['unmatched_no_answer']),
            'unmatched_has_answer': len(missing['unmatched_has_answer'])
        },
        'matches': matches,
        'missing_answers': missing
    }

    # å„²å­˜å ±å‘Š
    output_file = base_dir / 'DATASET_ANALYSIS_REPORT.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å ±å‘Šå·²å„²å­˜: {output_file}")

    # è¼¸å‡ºæ‘˜è¦
    print("\n" + "="*60)
    print("ğŸ“Š åˆ†ææ‘˜è¦")
    print("="*60)
    print(f"310 é¡Œç¸½æ•¸: {report['summary']['q310_total']}")
    print(f"719 é¡Œç¸½æ•¸: {report['summary']['q719_total']}")
    print(f"\nâœ… æ–‡å­—åŒ¹é…: {report['summary']['text_matched']} é¡Œ")
    print(f"   - éœ€è¦æ›´æ–°ç­”æ¡ˆ: {report['summary']['matched_need_update']} é¡Œ")
    print(f"\nâŒ 310 ä¸­ç„¡æ³•åœ¨ 719 æ‰¾åˆ°: {report['summary']['no_match_310']} é¡Œ")
    print(f"âŒ 719 ä¸­ç„¡æ³•åœ¨ 310 æ‰¾åˆ°: {report['summary']['no_match_719']} é¡Œ")
    print(f"   - ç„¡ç­”æ¡ˆ: {report['summary']['unmatched_no_answer']} é¡Œ")
    print(f"   - æœ‰ç­”æ¡ˆ: {report['summary']['unmatched_has_answer']} é¡Œ")

    # è¨ˆç®—éœ€è¦è™•ç†çš„ç¸½æ•¸
    total_need_answer = (
        report['summary']['matched_need_update'] +
        report['summary']['unmatched_no_answer']
    )
    print(f"\nğŸ¯ éœ€è¦æ‰¾ç­”æ¡ˆçš„é¡Œç›®ç¸½æ•¸: {total_need_answer} é¡Œ")
    print("="*60)

if __name__ == '__main__':
    main()
